{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data, Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "% matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (3.0.3) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\boto\\__init__.py:1142: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\c\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\boto\\pyami\\config.py:98: DeprecationWarning:\n",
      "\n",
      "invalid escape sequence \\s\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning:\n",
      "\n",
      "detected Windows; aliasing chunkize to chunkize_serial\n",
      "\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning:\n",
      "\n",
      "`scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "#To visualize errors:\n",
    "from prettytable import PrettyTable\n",
    "from collections import defaultdict\n",
    "\n",
    "#For topic modeling:\n",
    "from gensim.models.wrappers.ldamallet import LdaMallet\n",
    "from gensim.models import LdaModel, LsiModel\n",
    "from gensim.corpora import Dictionary\n",
    "mallet_path = './mallet-2.0.8/bin/mallet'\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file, test_file = './sampleData/sample_reviews_train.csv', './sampleData/sample_reviews_test.csv'\n",
    "reviews_train = pd.read_csv(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_classification(reviews_df, models, create_vectorizer, num_train=None, num_test=None, weight_fn=None):\n",
    "    \n",
    "    #Turn all the features into useful values:\n",
    "    train_df, test_df = train_test_split(reviews_df, test_size=0.2)\n",
    "    if num_train:\n",
    "        train_df = train_df.sample(num_train)\n",
    "    if num_test:\n",
    "        test_df = test_df.sample(num_test)\n",
    "    vectorizer = create_vectorizer()\n",
    "    vectorizer.fit(train_df)\n",
    "    print('Fit vectorizer')\n",
    "    train_vectors = vectorizer.transform(train_df)\n",
    "    test_vectors = vectorizer.transform(test_df)\n",
    "    print('Created vectors')\n",
    "    \n",
    "    #Create structures to print results of classification nicely:\n",
    "    train_scores, val_scores = defaultdict(list), defaultdict(list)\n",
    "    model_names = [str(model) for model in models]\n",
    "    score_tables = {name: PrettyTable() for name in model_names}\n",
    "    for table in score_tables.values():\n",
    "        table.field_names = ['Data', 'Train F1', 'Validation F1']\n",
    "        \n",
    "    #Here, we need to label each thing as a True or False:\n",
    "    trainLabels = np.ndarray(shape=(train_df.shape[0], 1), dtype=object)\n",
    "    testLabels = np.ndarray(shape=(test_df.shape[0], 1), dtype=object)\n",
    "    \n",
    "    train_vote_count = np.asarray(train_df['total_votes'])\n",
    "    test_vote_count = np.asarray(test_df['total_votes'])\n",
    "    \n",
    "    #For the train data:\n",
    "    falseIndices = np.where(train_vote_count == 0)\n",
    "    trueIndices = np.where(train_vote_count != 0)\n",
    "    \n",
    "    np.put(trainLabels, falseIndices, False)\n",
    "    np.put(trainLabels, trueIndices, True)\n",
    "    \n",
    "    #For the test data:\n",
    "    falseIndices = np.where(test_vote_count == 0)\n",
    "    trueIndices = np.where(test_vote_count != 0)\n",
    "    \n",
    "    np.put(testLabels, falseIndices, False)\n",
    "    np.put(testLabels, trueIndices, True)\n",
    "    \n",
    "    #To prevent unknown type errors:\n",
    "    trainLabels = trainLabels.astype(bool)\n",
    "    testLabels = testLabels.astype(bool)\n",
    "    \n",
    "    #Now we do the classification:\n",
    "    #Note that the flatten() was added just to avoid an annoying warning about column vectors\n",
    "    for model in models:\n",
    "        if (model == LinearSVC):\n",
    "            createdModel = model(max_iter=10000)\n",
    "        else:\n",
    "            createdModel = model()\n",
    "            \n",
    "        if weight_fn:\n",
    "            createdModel.fit(train_vectors, trainLabels.flatten(), sample_weight=weight_fn(train_df))\n",
    "        else:\n",
    "            createdModel.fit(train_vectors, trainLabels.flatten())\n",
    "        print('Fit model')\n",
    "    \n",
    "        #Get error stats:\n",
    "        #First the predictions:\n",
    "        train_predictions = createdModel.predict(train_vectors)\n",
    "        test_predictions = createdModel.predict(test_vectors)\n",
    "\n",
    "        #Now the actual scores:\n",
    "        train_f1 = f1_score(trainLabels, train_predictions, pos_label=True)\n",
    "        val_f1 = f1_score(testLabels, test_predictions, pos_label=False)\n",
    "\n",
    "        #Now build the tables:\n",
    "        #name = model.__class__.__name__\n",
    "        name = str(model)\n",
    "        score_tables[name].add_row(['Placeholder', f'{train_f1:.3f}', f'{val_f1:.3f}'])\n",
    "        train_scores[name].append(train_f1)\n",
    "        val_scores[name].append(val_f1)\n",
    "\n",
    "    #Give a more general metric:\n",
    "    for name, table in score_tables.items():\n",
    "        table.add_row(\n",
    "            ('Average +/- std.dev',\n",
    "             f'{np.mean(train_scores[name]):.3f} +/- {np.std(train_scores[name]):.3f}', \n",
    "             f'{np.mean(val_scores[name]):.3f} +/- {np.std(val_scores[name]):.3f}'\n",
    "            ))\n",
    "    for name, table in score_tables.items():\n",
    "        print(f\"\\n{name}\\n{'-' * len(name)}\")\n",
    "        print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PandasCountVectorizer(TransformerMixin):\n",
    "    # A bag-of-words vectorizer that works with an entire DataFrame as input.\n",
    "    # This lets me create a combined feature pipeline with multiple feature vectorizers\n",
    "    # where I don't have to worry about passing different columns to each.\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.vectorizer = CountVectorizer(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X, *args, **kwargs):\n",
    "        self.vectorizer.fit(X['text'].values)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.vectorizer.transform(X['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomFeatures(TransformerMixin):\n",
    "    # A feature vectorizer that creates features based on length, readability and review stars.\n",
    "    # Use this to add other new custom features (like sentiment and user history maybe)\n",
    "    # Btw, I think sentiment might not help much - intuitively it seems like it'd be highly correlated with \n",
    "    # review stars - 5 star rating - likely positive; 1 star rating - likely negative.\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def _review_features(self, review):\n",
    "        return [\n",
    "            len(review.text),\n",
    "            review.stars,\n",
    "            review.coleman_liau_index,\n",
    "            review.automated_readability_index,\n",
    "            review.dale_chall_readability_score,\n",
    "            review.linsear_write_formula,\n",
    "            review.gunning_fog,\n",
    "            review.flesch_reading_ease,\n",
    "        ]\n",
    "\n",
    "    def transform(self, reviews):\n",
    "        return np.array([self._review_features(r) for r in reviews.itertuples()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [RandomForestClassifier, LogisticRegression, XGBClassifier, LinearSVC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit vectorizer\n",
      "Created vectors\n",
      "Fit model\n",
      "Fit model\n",
      "Fit model\n",
      "Fit model\n",
      "\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "--------------------------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.983      |      0.652      |\n",
      "| Average +/- std.dev | 0.983 +/- 0.000 | 0.652 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n",
      "\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "----------------------------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.607      |      0.674      |\n",
      "| Average +/- std.dev | 0.607 +/- 0.000 | 0.674 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n",
      "\n",
      "<class 'xgboost.sklearn.XGBClassifier'>\n",
      "---------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.604      |      0.676      |\n",
      "| Average +/- std.dev | 0.604 +/- 0.000 | 0.676 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "---------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.599      |      0.677      |\n",
      "| Average +/- std.dev | 0.599 +/- 0.000 | 0.677 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning:\n",
      "\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordinary Classification with bag of words with basic filtering\n",
    "bow_vectorizer_fn = lambda: PandasCountVectorizer(min_df=10, max_df=0.75, max_features=1000)\n",
    "\n",
    "#Now we classify with each model:\n",
    "run_classification(reviews_train, models, bow_vectorizer_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here for how I'm calculating baseline F1: https://stats.stackexchange.com/questions/217376/intuition-about-f1-score\n",
    "\n",
    "So first let's get some metrics/methodology descriptions out of the way: I went with Munmun's suggestion of just predict class based on whether any votes were given to a review. This avoids the whole threshold issue and actually gives about a 40/60 split on the data (40% is classified as \"endorsed\" or \"True\" in the case of the above code). For baseline metrics, we can achieve a 0.65 F1 score on the train set and 0.651 F1 score on the validation set, should we predict all reviews to have social endorsement.\n",
    "\n",
    "Taking those values into consideration, the models we are trying do not do very well. The Random Forest seems to overfit to the training data, as it gets a 0.985 F1 score and a 0.65 F1 score on the validation set, so it did very badly. LogisticRegression gets a validation F1 score of 0.683, which sounds better than the forest, until you realize that its training F1 score is 0.61, so that's actually worse than the F1 baseline. It's a similar story with the XGB Classifier, and SVC is having problems, so...\n",
    "\n",
    "In any case, the bag of words feature alone does not help and can actually make things worse. It does not bode well for classification unfortunately, but let's keep going and see what we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add readability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coleman_liau_index\n",
      "automated_readability_index\n",
      "dale_chall_readability_score\n",
      "linsear_write_formula\n",
      "gunning_fog\n",
      "flesch_reading_ease\n"
     ]
    }
   ],
   "source": [
    "readability_fns = [\n",
    "    textstat.coleman_liau_index,\n",
    "    textstat.automated_readability_index,\n",
    "    textstat.dale_chall_readability_score,\n",
    "    textstat.linsear_write_formula,\n",
    "    textstat.gunning_fog,\n",
    "    textstat.flesch_reading_ease,\n",
    "]\n",
    "for fn in readability_fns:\n",
    "    print(fn.__name__)\n",
    "    reviews_train[fn.__name__] = reviews_train['text'].apply(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_train.to_csv(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit vectorizer\n",
      "Created vectors\n",
      "Fit model\n",
      "Fit model\n",
      "Fit model\n",
      "Fit model\n",
      "\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "--------------------------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.983      |      0.649      |\n",
      "| Average +/- std.dev | 0.983 +/- 0.000 | 0.649 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n",
      "\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "----------------------------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.615      |      0.684      |\n",
      "| Average +/- std.dev | 0.615 +/- 0.000 | 0.684 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n",
      "\n",
      "<class 'xgboost.sklearn.XGBClassifier'>\n",
      "---------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.630      |      0.682      |\n",
      "| Average +/- std.dev | 0.630 +/- 0.000 | 0.682 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "---------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.000      |      0.686      |\n",
      "| Average +/- std.dev | 0.000 +/- 0.000 | 0.686 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning:\n",
      "\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification with bag-of-words + custom features (length, review stars, different readability scores)\n",
    "bow_readability_vectorizer_fn = lambda: FeatureUnion([\n",
    "    ('bow', PandasCountVectorizer(min_df=10, max_df=0.75, max_features=1000)),\n",
    "    ('custom', CustomFeatures()),\n",
    "])\n",
    "run_classification(reviews_train, models, bow_readability_vectorizer_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the logistic regression seemed to get some benefit out of adding these features, but that still didn't really give us much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I tried weighted classification - since we have lots of data with near-zero votes and very few with high votes, I decided to add a weight to each training example proportional to its vote count. I experimented with different formulae - nothing helps much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit vectorizer\n",
      "Created vectors\n",
      "Fit model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning:\n",
      "\n",
      "Liblinear failed to converge, increase the number of iterations.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model\n",
      "Fit model\n",
      "Fit model\n",
      "\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "--------------------------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.982      |      0.658      |\n",
      "| Average +/- std.dev | 0.982 +/- 0.000 | 0.658 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n",
      "\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "----------------------------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.649      |      0.648      |\n",
      "| Average +/- std.dev | 0.649 +/- 0.000 | 0.648 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n",
      "\n",
      "<class 'xgboost.sklearn.XGBClassifier'>\n",
      "---------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.659      |      0.636      |\n",
      "| Average +/- std.dev | 0.659 +/- 0.000 | 0.636 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "---------------------------------------\n",
      "+---------------------+-----------------+-----------------+\n",
      "|         Data        |     Train F1    |  Validation F1  |\n",
      "+---------------------+-----------------+-----------------+\n",
      "|     Placeholder     |      0.459      |      0.704      |\n",
      "| Average +/- std.dev | 0.459 +/- 0.000 | 0.704 +/- 0.000 |\n",
      "+---------------------+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "weight_by_votes = lambda df: 1 + df.total_votes.values/10\n",
    "run_classification(reviews_train, models, bow_readability_vectorizer_fn, weight_fn=weight_by_votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighting certain samples somehow made things worse, and I'm not sure what that really means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    def __init__(self, texts):\n",
    "        dictionary = Dictionary([self.text_to_tokens(text) for text in texts])\n",
    "        dictionary.filter_extremes(no_below=5, no_above=0.6)\n",
    "        self._dictionary = dictionary\n",
    "        self._texts = texts\n",
    "\n",
    "    def text_to_tokens(self, text):\n",
    "        return [w for w in re.split('\\W', text) if w]\n",
    "    \n",
    "    def doc2bow(self, text):\n",
    "        return self._dictionary.doc2bow(self.text_to_tokens(text))\n",
    "\n",
    "    def __iter__(self):\n",
    "        for text in self._texts:\n",
    "            yield self.doc2bow(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(reviews_train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_model = LdaModel(corpus, id2word=corpus._dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi_model = LsiModel(corpus, id2word=corpus._dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TopicModelVectorizer(TransformerMixin):\n",
    "    def __init__(self, model, corpus):\n",
    "        self._model = model\n",
    "        self._corpus = corpus\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self\n",
    "    \n",
    "    def doc_topic_vector(self, doc):\n",
    "        topic_scores = self._model[self._corpus.doc2bow(doc)]\n",
    "        topic_vector = [0 for _ in range(self._model.num_topics)]\n",
    "        for topic_id, score in topic_scores:\n",
    "            topic_vector[topic_id] = score\n",
    "        return topic_vector\n",
    "\n",
    "    def transform(self, docs):\n",
    "        return np.array([self.doc_topic_vector(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_topic_vectorizer_creator(model, corpus):\n",
    "    return lambda: TopicModelVectorizer(model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we classify with each model using topic modeling:\n",
    "\n",
    "run_classification(reviews_train, models, create_topic_vectorizer_creator(topic_model, corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Correlation Analysis\n",
    "\n",
    "Not sure why this isn't working, but I think it has to do with the fact that the matrix is sparse.\n",
    "Only 8 features for nearly 40k reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit vectorizer\n",
      "Created vectors\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('Automated Readability Index', 'occurred at index Automated Readability Index')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5085)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas\\_libs\\hashtable.c:13892)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-ed29d2116903>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviewFeat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0msplitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mreviewFeat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreviewFeat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# reviewFeat = reviewFeat.applymap(split(\"\\t\")[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   4358\u001b[0m                         \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m                         \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4360\u001b[1;33m                         ignore_failures=ignore_failures)\n\u001b[0m\u001b[0;32m   4361\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4362\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   4454\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4455\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4456\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4457\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4458\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-ed29d2116903>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviewFeat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0msplitter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mreviewFeat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreviewFeat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   2426\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2427\u001b[0m             return self._engine.get_value(s, k,\n\u001b[1;32m-> 2428\u001b[1;33m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[0;32m   2429\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2430\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas\\_libs\\index.c:4363)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value (pandas\\_libs\\index.c:4046)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5169)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ('Automated Readability Index', 'occurred at index Automated Readability Index')"
     ]
    }
   ],
   "source": [
    "#Referenced https://stackoverflow.com/questions/36108377/how-to-use-the-split-function-on-every-row-in-a-dataframe-in-python\n",
    "#Referenced https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_dataframes/\n",
    "#Referenced https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
    "\n",
    "#We know what vectorizer function has all the features we want, so create those features:\n",
    "vectorizer = bow_readability_vectorizer_fn()\n",
    "vectorizer.fit(reviews_train)\n",
    "print('Fit vectorizer')\n",
    "review_vectors = vectorizer.transform(reviews_train)\n",
    "print('Created vectors')\n",
    "\n",
    "#Now we want to find correlations between these features:\n",
    "#I am ignoring bag of words, as I am not sure how correlation will work there\n",
    "reviewFeat = review_vectors[:, 1000:1008]\n",
    "reviewFeat = pd.DataFrame({\"Length\": list(reviewFeat[:,0]), \"Star Count\": list(reviewFeat[:,1]),\n",
    "                          \"Coleman-Liau Index\": list(reviewFeat[:,2]),\n",
    "                           \"Automated Readability Index\": list(reviewFeat[:,3]),\n",
    "                           \"Dale Chall Readability Score\": list(reviewFeat[:,4]),\n",
    "                           \"Linesear Write Formula\": list(reviewFeat[:,5]),\n",
    "                           \"Gunning-Fog Score\": list(reviewFeat[:,6]),\n",
    "                           \"Flesch Reading Ease\": list(reviewFeat[:,7])})\n",
    "\n",
    "reviewFeat = reviewFeat.astype(str)\n",
    "\n",
    "for col in list(reviewFeat.columns.values):\n",
    "    splitter = lambda x: x[col].split(\"\\t\")[1]\n",
    "    reviewFeat[col] = reviewFeat.apply(splitter, axis=0)\n",
    "    \n",
    "# reviewFeat = reviewFeat.applymap(split(\"\\t\")[1])\n",
    "print(reviewFeat.head())\n",
    "# reviewFeat = pd.DataFrame(data=reviewFeat.ravel(), index=np.arange(reviewFeat.shape[0]),\n",
    "#                                                            columns=[\"Length\", \"Star Count\", \"Coleman-Liau Index\",\n",
    "#                                                                     \"Automated Readability Index\",\n",
    "#                                                                     \"Dale Chall Readability Score\",\n",
    "#                                                                     \"Linesear Write Formula\",\n",
    "#                                                                     \"Gunning-Fog Score\",\n",
    "#                                                                     \"Flesch Reading Ease\"])\n",
    "sns.heatmap(data=reviewFeat.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
